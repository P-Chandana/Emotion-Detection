{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":12,"outputs":[{"output_type":"stream","text":"/kaggle/input/haarcascade-frontalface/haarcascade_frontalface_default.xml\n/kaggle/input/facial-expression-recognitionferchallenge/Submission.csv\n/kaggle/input/facial-expression-recognitionferchallenge/fer2013/fer2013/README\n/kaggle/input/facial-expression-recognitionferchallenge/fer2013/fer2013/fer2013.csv\n/kaggle/input/facial-expression-recognitionferchallenge/fer2013/fer2013/fer2013.bib\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import sys, os\nimport pandas as pd\nimport numpy as np\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D\nfrom keras.losses import categorical_crossentropy\nfrom keras.optimizers import Adam\nfrom keras.regularizers import l2\nfrom keras.utils import np_utils","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndata = pd.read_csv(\"/kaggle/input/facial-expression-recognitionferchallenge/fer2013/fer2013/fer2013.csv\")","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,train_y,X_test,test_y=[],[],[],[]\n\nfor index, row in data.iterrows():\n    val=row['pixels'].split(\" \")\n    try:\n        if 'Training' in row['Usage']:\n           X_train.append(np.array(val,'float32'))\n           train_y.append(row['emotion'])\n        elif 'PublicTest' in row['Usage']:\n           X_test.append(np.array(val,'float32'))\n           test_y.append(row['emotion'])\n    except:\n        print(f\"error occured at index :{index} and row:{row}\")\n\n\nnum_features = 64\nnum_labels = 7\nbatch_size = 64\nepochs = 200\nwidth, height = 48, 48\n\n\nX_train = np.array(X_train,'float32')\ntrain_y = np.array(train_y,'float32')\nX_test = np.array(X_test,'float32')\ntest_y = np.array(test_y,'float32')\n\ntrain_y=np_utils.to_categorical(train_y, num_classes=num_labels)\ntest_y=np_utils.to_categorical(test_y, num_classes=num_labels)\n\n#cannot produce\n#normalizing data between oand 1\nX_train -= np.mean(X_train, axis=0)\nX_train /= np.std(X_train, axis=0)\n\nX_test -= np.mean(X_test, axis=0)\nX_test /= np.std(X_test, axis=0)\n\nX_train = X_train.reshape(X_train.shape[0], 48, 48, 1)\n\nX_test = X_test.reshape(X_test.shape[0], 48, 48, 1)\n\n# print(f\"shape:{X_train.shape}\")\n##designing the cnn\n#1st convolution layer\nmodel = Sequential()\n\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(X_train.shape[1:])))\nmodel.add(Conv2D(64,kernel_size= (3, 3), activation='relu'))\n# model.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\nmodel.add(Dropout(0.5))\n\n#2nd convolution layer\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\n# model.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\nmodel.add(Dropout(0.5))\n\n#3rd convolution layer\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\n# model.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n\nmodel.add(Flatten())\n\n#fully connected neural networks\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(num_labels, activation='softmax'))\n\n# model.summary()\n\n#Compliling the model\nmodel.compile(loss=categorical_crossentropy,\n              optimizer=Adam(),\n              metrics=['accuracy'])\n\n#Training the model\nmodel.fit(X_train, train_y,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(X_test, test_y),\n          shuffle=True)\n","execution_count":4,"outputs":[{"output_type":"stream","text":"Epoch 1/200\n449/449 [==============================] - 6s 14ms/step - loss: 1.7156 - accuracy: 0.2975 - val_loss: 1.6297 - val_accuracy: 0.3536\nEpoch 2/200\n449/449 [==============================] - 6s 13ms/step - loss: 1.5208 - accuracy: 0.4018 - val_loss: 1.3936 - val_accuracy: 0.4558\nEpoch 3/200\n449/449 [==============================] - 6s 12ms/step - loss: 1.4125 - accuracy: 0.4515 - val_loss: 1.3114 - val_accuracy: 0.4887\nEpoch 4/200\n449/449 [==============================] - 6s 12ms/step - loss: 1.3443 - accuracy: 0.4804 - val_loss: 1.2644 - val_accuracy: 0.5166\nEpoch 5/200\n449/449 [==============================] - 6s 12ms/step - loss: 1.2958 - accuracy: 0.4993 - val_loss: 1.2707 - val_accuracy: 0.5049\nEpoch 6/200\n449/449 [==============================] - 6s 12ms/step - loss: 1.2586 - accuracy: 0.5188 - val_loss: 1.2151 - val_accuracy: 0.5308\nEpoch 7/200\n449/449 [==============================] - 6s 12ms/step - loss: 1.2241 - accuracy: 0.5306 - val_loss: 1.2159 - val_accuracy: 0.5269\nEpoch 8/200\n449/449 [==============================] - 6s 12ms/step - loss: 1.1898 - accuracy: 0.5460 - val_loss: 1.2004 - val_accuracy: 0.5419\nEpoch 9/200\n449/449 [==============================] - 6s 12ms/step - loss: 1.1691 - accuracy: 0.5541 - val_loss: 1.1831 - val_accuracy: 0.5450\nEpoch 10/200\n449/449 [==============================] - 6s 13ms/step - loss: 1.1451 - accuracy: 0.5640 - val_loss: 1.1673 - val_accuracy: 0.5522\nEpoch 11/200\n449/449 [==============================] - 6s 13ms/step - loss: 1.1251 - accuracy: 0.5705 - val_loss: 1.1784 - val_accuracy: 0.5606\nEpoch 12/200\n449/449 [==============================] - 6s 13ms/step - loss: 1.1047 - accuracy: 0.5760 - val_loss: 1.1416 - val_accuracy: 0.5659\nEpoch 13/200\n449/449 [==============================] - 6s 13ms/step - loss: 1.0798 - accuracy: 0.5877 - val_loss: 1.1804 - val_accuracy: 0.5486\nEpoch 14/200\n449/449 [==============================] - 6s 12ms/step - loss: 1.0642 - accuracy: 0.5923 - val_loss: 1.1461 - val_accuracy: 0.5754\nEpoch 15/200\n449/449 [==============================] - 6s 12ms/step - loss: 1.0461 - accuracy: 0.6008 - val_loss: 1.1354 - val_accuracy: 0.5715\nEpoch 16/200\n449/449 [==============================] - 6s 13ms/step - loss: 1.0309 - accuracy: 0.6078 - val_loss: 1.1494 - val_accuracy: 0.5687\nEpoch 17/200\n449/449 [==============================] - 6s 12ms/step - loss: 1.0147 - accuracy: 0.6122 - val_loss: 1.1469 - val_accuracy: 0.5701\nEpoch 18/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.9943 - accuracy: 0.6225 - val_loss: 1.1740 - val_accuracy: 0.5623\nEpoch 19/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.9734 - accuracy: 0.6317 - val_loss: 1.1910 - val_accuracy: 0.5690\nEpoch 20/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.9573 - accuracy: 0.6356 - val_loss: 1.1760 - val_accuracy: 0.5695\nEpoch 21/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.9447 - accuracy: 0.6387 - val_loss: 1.1583 - val_accuracy: 0.5748\nEpoch 22/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.9280 - accuracy: 0.6462 - val_loss: 1.1696 - val_accuracy: 0.5765\nEpoch 23/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.9101 - accuracy: 0.6570 - val_loss: 1.1581 - val_accuracy: 0.5768\nEpoch 24/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.8891 - accuracy: 0.6655 - val_loss: 1.1502 - val_accuracy: 0.5874\nEpoch 25/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.8763 - accuracy: 0.6683 - val_loss: 1.1588 - val_accuracy: 0.5795\nEpoch 26/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.8598 - accuracy: 0.6727 - val_loss: 1.2025 - val_accuracy: 0.5784\nEpoch 27/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.8395 - accuracy: 0.6822 - val_loss: 1.2058 - val_accuracy: 0.5681\nEpoch 28/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.8332 - accuracy: 0.6818 - val_loss: 1.1973 - val_accuracy: 0.5823\nEpoch 29/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.8205 - accuracy: 0.6904 - val_loss: 1.2084 - val_accuracy: 0.5770\nEpoch 30/200\n449/449 [==============================] - 5s 12ms/step - loss: 0.8054 - accuracy: 0.6960 - val_loss: 1.2256 - val_accuracy: 0.5804\nEpoch 31/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.7920 - accuracy: 0.7001 - val_loss: 1.2073 - val_accuracy: 0.5857\nEpoch 32/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.7763 - accuracy: 0.7077 - val_loss: 1.2389 - val_accuracy: 0.5731\nEpoch 33/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.7542 - accuracy: 0.7136 - val_loss: 1.2514 - val_accuracy: 0.5687\nEpoch 34/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.7477 - accuracy: 0.7213 - val_loss: 1.2858 - val_accuracy: 0.5770\nEpoch 35/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.7465 - accuracy: 0.7193 - val_loss: 1.2547 - val_accuracy: 0.5729\nEpoch 36/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.7326 - accuracy: 0.7260 - val_loss: 1.3080 - val_accuracy: 0.5851\nEpoch 37/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.7179 - accuracy: 0.7315 - val_loss: 1.2872 - val_accuracy: 0.5882\nEpoch 38/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.7049 - accuracy: 0.7338 - val_loss: 1.3209 - val_accuracy: 0.5787\nEpoch 39/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.6938 - accuracy: 0.7379 - val_loss: 1.2605 - val_accuracy: 0.5740\nEpoch 40/200\n449/449 [==============================] - 5s 12ms/step - loss: 0.6869 - accuracy: 0.7433 - val_loss: 1.2684 - val_accuracy: 0.5790\nEpoch 41/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.6742 - accuracy: 0.7499 - val_loss: 1.3020 - val_accuracy: 0.5787\nEpoch 42/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.6719 - accuracy: 0.7491 - val_loss: 1.3047 - val_accuracy: 0.5784\nEpoch 43/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.6566 - accuracy: 0.7582 - val_loss: 1.2606 - val_accuracy: 0.5924\nEpoch 44/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.6401 - accuracy: 0.7629 - val_loss: 1.3049 - val_accuracy: 0.5857\nEpoch 45/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.6512 - accuracy: 0.7558 - val_loss: 1.3871 - val_accuracy: 0.5748\nEpoch 46/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.6376 - accuracy: 0.7638 - val_loss: 1.3766 - val_accuracy: 0.5807\nEpoch 47/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.6240 - accuracy: 0.7663 - val_loss: 1.3571 - val_accuracy: 0.5770\nEpoch 48/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.6143 - accuracy: 0.7716 - val_loss: 1.3315 - val_accuracy: 0.5809\nEpoch 49/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.6070 - accuracy: 0.7756 - val_loss: 1.3602 - val_accuracy: 0.5851\nEpoch 50/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.5990 - accuracy: 0.7760 - val_loss: 1.3184 - val_accuracy: 0.5893\nEpoch 51/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.5974 - accuracy: 0.7821 - val_loss: 1.3716 - val_accuracy: 0.5840\nEpoch 52/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.5890 - accuracy: 0.7836 - val_loss: 1.3848 - val_accuracy: 0.5865\nEpoch 53/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.5808 - accuracy: 0.7873 - val_loss: 1.3708 - val_accuracy: 0.5793\nEpoch 54/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.5714 - accuracy: 0.7899 - val_loss: 1.4176 - val_accuracy: 0.5793\nEpoch 55/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.5685 - accuracy: 0.7906 - val_loss: 1.4088 - val_accuracy: 0.5787\nEpoch 56/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.5696 - accuracy: 0.7919 - val_loss: 1.4313 - val_accuracy: 0.5834\nEpoch 57/200\n","name":"stdout"},{"output_type":"stream","text":"449/449 [==============================] - 6s 12ms/step - loss: 0.5532 - accuracy: 0.7959 - val_loss: 1.4618 - val_accuracy: 0.5784\nEpoch 58/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.5507 - accuracy: 0.7988 - val_loss: 1.4273 - val_accuracy: 0.5729\nEpoch 59/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.5485 - accuracy: 0.8000 - val_loss: 1.4110 - val_accuracy: 0.5818\nEpoch 60/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.5423 - accuracy: 0.7991 - val_loss: 1.4823 - val_accuracy: 0.5729\nEpoch 61/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.5383 - accuracy: 0.8036 - val_loss: 1.4311 - val_accuracy: 0.5790\nEpoch 62/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.5291 - accuracy: 0.8050 - val_loss: 1.4583 - val_accuracy: 0.5832\nEpoch 63/200\n449/449 [==============================] - 5s 12ms/step - loss: 0.5277 - accuracy: 0.8098 - val_loss: 1.4499 - val_accuracy: 0.5823\nEpoch 64/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.5243 - accuracy: 0.8099 - val_loss: 1.4383 - val_accuracy: 0.5815\nEpoch 65/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.5079 - accuracy: 0.8151 - val_loss: 1.4816 - val_accuracy: 0.5821\nEpoch 66/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.5119 - accuracy: 0.8138 - val_loss: 1.4731 - val_accuracy: 0.5731\nEpoch 67/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.5089 - accuracy: 0.8159 - val_loss: 1.4437 - val_accuracy: 0.5754\nEpoch 68/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.4900 - accuracy: 0.8235 - val_loss: 1.4538 - val_accuracy: 0.5787\nEpoch 69/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.5006 - accuracy: 0.8198 - val_loss: 1.4713 - val_accuracy: 0.5826\nEpoch 70/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.4917 - accuracy: 0.8226 - val_loss: 1.5431 - val_accuracy: 0.5717\nEpoch 71/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.4886 - accuracy: 0.8230 - val_loss: 1.4748 - val_accuracy: 0.5812\nEpoch 72/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.4813 - accuracy: 0.8242 - val_loss: 1.5317 - val_accuracy: 0.5793\nEpoch 73/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.4857 - accuracy: 0.8238 - val_loss: 1.5251 - val_accuracy: 0.5704\nEpoch 74/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.4711 - accuracy: 0.8289 - val_loss: 1.5900 - val_accuracy: 0.5720\nEpoch 75/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.4689 - accuracy: 0.8313 - val_loss: 1.5540 - val_accuracy: 0.5826\nEpoch 76/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.4706 - accuracy: 0.8299 - val_loss: 1.5604 - val_accuracy: 0.5801\nEpoch 77/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.4646 - accuracy: 0.8325 - val_loss: 1.5859 - val_accuracy: 0.5743\nEpoch 78/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.4593 - accuracy: 0.8342 - val_loss: 1.5380 - val_accuracy: 0.5826\nEpoch 79/200\n449/449 [==============================] - 5s 12ms/step - loss: 0.4484 - accuracy: 0.8399 - val_loss: 1.5594 - val_accuracy: 0.5712\nEpoch 80/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.4523 - accuracy: 0.8392 - val_loss: 1.6274 - val_accuracy: 0.5729\nEpoch 81/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.4496 - accuracy: 0.8395 - val_loss: 1.5923 - val_accuracy: 0.5787\nEpoch 82/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.4444 - accuracy: 0.8422 - val_loss: 1.5686 - val_accuracy: 0.5768\nEpoch 83/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.4384 - accuracy: 0.8440 - val_loss: 1.6415 - val_accuracy: 0.5790\nEpoch 84/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.4403 - accuracy: 0.8405 - val_loss: 1.5293 - val_accuracy: 0.5773\nEpoch 85/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.4384 - accuracy: 0.8433 - val_loss: 1.6062 - val_accuracy: 0.5751\nEpoch 86/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.4196 - accuracy: 0.8503 - val_loss: 1.6376 - val_accuracy: 0.5695\nEpoch 87/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.4284 - accuracy: 0.8458 - val_loss: 1.6741 - val_accuracy: 0.5860\nEpoch 88/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.4261 - accuracy: 0.8471 - val_loss: 1.6549 - val_accuracy: 0.5673\nEpoch 89/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.4201 - accuracy: 0.8507 - val_loss: 1.6565 - val_accuracy: 0.5690\nEpoch 90/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.4187 - accuracy: 0.8518 - val_loss: 1.6750 - val_accuracy: 0.5793\nEpoch 91/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.4172 - accuracy: 0.8537 - val_loss: 1.6431 - val_accuracy: 0.5737\nEpoch 92/200\n449/449 [==============================] - 5s 12ms/step - loss: 0.4116 - accuracy: 0.8529 - val_loss: 1.6455 - val_accuracy: 0.5673\nEpoch 93/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.4112 - accuracy: 0.8552 - val_loss: 1.6620 - val_accuracy: 0.5821\nEpoch 94/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.4054 - accuracy: 0.8572 - val_loss: 1.6439 - val_accuracy: 0.5651\nEpoch 95/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.4095 - accuracy: 0.8570 - val_loss: 1.6398 - val_accuracy: 0.5818\nEpoch 96/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.4007 - accuracy: 0.8556 - val_loss: 1.6259 - val_accuracy: 0.5809\nEpoch 97/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3954 - accuracy: 0.8624 - val_loss: 1.7113 - val_accuracy: 0.5784\nEpoch 98/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3970 - accuracy: 0.8587 - val_loss: 1.6912 - val_accuracy: 0.5787\nEpoch 99/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.4000 - accuracy: 0.8595 - val_loss: 1.6337 - val_accuracy: 0.5756\nEpoch 100/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3952 - accuracy: 0.8623 - val_loss: 1.6694 - val_accuracy: 0.5818\nEpoch 101/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3892 - accuracy: 0.8641 - val_loss: 1.7149 - val_accuracy: 0.5807\nEpoch 102/200\n449/449 [==============================] - 5s 12ms/step - loss: 0.3932 - accuracy: 0.8615 - val_loss: 1.7119 - val_accuracy: 0.5840\nEpoch 103/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3902 - accuracy: 0.8627 - val_loss: 1.6659 - val_accuracy: 0.5776\nEpoch 104/200\n449/449 [==============================] - 5s 12ms/step - loss: 0.3956 - accuracy: 0.8630 - val_loss: 1.6659 - val_accuracy: 0.5790\nEpoch 105/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3854 - accuracy: 0.8640 - val_loss: 1.6727 - val_accuracy: 0.5834\nEpoch 106/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3832 - accuracy: 0.8687 - val_loss: 1.7035 - val_accuracy: 0.5754\nEpoch 107/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3713 - accuracy: 0.8709 - val_loss: 1.7402 - val_accuracy: 0.5765\nEpoch 108/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3839 - accuracy: 0.8669 - val_loss: 1.6858 - val_accuracy: 0.5782\nEpoch 109/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3820 - accuracy: 0.8645 - val_loss: 1.6512 - val_accuracy: 0.5731\nEpoch 110/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3688 - accuracy: 0.8716 - val_loss: 1.7075 - val_accuracy: 0.5837\nEpoch 111/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3699 - accuracy: 0.8680 - val_loss: 1.6776 - val_accuracy: 0.5773\nEpoch 112/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3744 - accuracy: 0.8714 - val_loss: 1.6572 - val_accuracy: 0.5690\nEpoch 113/200\n","name":"stdout"},{"output_type":"stream","text":"449/449 [==============================] - 6s 12ms/step - loss: 0.3813 - accuracy: 0.8667 - val_loss: 1.6439 - val_accuracy: 0.5698\nEpoch 114/200\n449/449 [==============================] - 5s 12ms/step - loss: 0.3726 - accuracy: 0.8694 - val_loss: 1.6955 - val_accuracy: 0.5759\nEpoch 115/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3625 - accuracy: 0.8758 - val_loss: 1.6888 - val_accuracy: 0.5795\nEpoch 116/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3594 - accuracy: 0.8751 - val_loss: 1.7989 - val_accuracy: 0.5690\nEpoch 117/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3709 - accuracy: 0.8725 - val_loss: 1.7555 - val_accuracy: 0.5645\nEpoch 118/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3655 - accuracy: 0.8735 - val_loss: 1.7687 - val_accuracy: 0.5762\nEpoch 119/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3657 - accuracy: 0.8725 - val_loss: 1.7433 - val_accuracy: 0.5737\nEpoch 120/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3636 - accuracy: 0.8734 - val_loss: 1.7257 - val_accuracy: 0.5701\nEpoch 121/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3612 - accuracy: 0.8752 - val_loss: 1.7762 - val_accuracy: 0.5779\nEpoch 122/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3585 - accuracy: 0.8756 - val_loss: 1.7601 - val_accuracy: 0.5798\nEpoch 123/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3578 - accuracy: 0.8769 - val_loss: 1.7403 - val_accuracy: 0.5698\nEpoch 124/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3479 - accuracy: 0.8796 - val_loss: 1.6961 - val_accuracy: 0.5815\nEpoch 125/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3778 - accuracy: 0.8694 - val_loss: 1.7347 - val_accuracy: 0.5692\nEpoch 126/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3483 - accuracy: 0.8791 - val_loss: 1.7077 - val_accuracy: 0.5743\nEpoch 127/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3484 - accuracy: 0.8794 - val_loss: 1.8308 - val_accuracy: 0.5717\nEpoch 128/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3650 - accuracy: 0.8738 - val_loss: 1.7525 - val_accuracy: 0.5731\nEpoch 129/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3507 - accuracy: 0.8806 - val_loss: 1.7364 - val_accuracy: 0.5790\nEpoch 130/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3440 - accuracy: 0.8829 - val_loss: 1.8245 - val_accuracy: 0.5807\nEpoch 131/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3452 - accuracy: 0.8825 - val_loss: 1.8074 - val_accuracy: 0.5748\nEpoch 132/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3378 - accuracy: 0.8824 - val_loss: 1.7482 - val_accuracy: 0.5651\nEpoch 133/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3420 - accuracy: 0.8827 - val_loss: 1.8211 - val_accuracy: 0.5854\nEpoch 134/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3322 - accuracy: 0.8888 - val_loss: 1.7693 - val_accuracy: 0.5879\nEpoch 135/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3320 - accuracy: 0.8869 - val_loss: 1.8166 - val_accuracy: 0.5807\nEpoch 136/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3242 - accuracy: 0.8902 - val_loss: 1.8194 - val_accuracy: 0.5793\nEpoch 137/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3440 - accuracy: 0.8815 - val_loss: 1.7765 - val_accuracy: 0.5754\nEpoch 138/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3305 - accuracy: 0.8858 - val_loss: 1.8377 - val_accuracy: 0.5812\nEpoch 139/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3273 - accuracy: 0.8883 - val_loss: 1.7431 - val_accuracy: 0.5862\nEpoch 140/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3330 - accuracy: 0.8882 - val_loss: 1.8001 - val_accuracy: 0.5759\nEpoch 141/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3351 - accuracy: 0.8841 - val_loss: 1.7785 - val_accuracy: 0.5815\nEpoch 142/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3323 - accuracy: 0.8854 - val_loss: 1.7810 - val_accuracy: 0.5821\nEpoch 143/200\n449/449 [==============================] - 5s 12ms/step - loss: 0.3397 - accuracy: 0.8849 - val_loss: 1.8296 - val_accuracy: 0.5756\nEpoch 144/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3196 - accuracy: 0.8916 - val_loss: 1.8698 - val_accuracy: 0.5698\nEpoch 145/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3372 - accuracy: 0.8852 - val_loss: 1.8218 - val_accuracy: 0.5734\nEpoch 146/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3192 - accuracy: 0.8921 - val_loss: 1.8311 - val_accuracy: 0.5737\nEpoch 147/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3188 - accuracy: 0.8921 - val_loss: 1.8268 - val_accuracy: 0.5743\nEpoch 148/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3234 - accuracy: 0.8913 - val_loss: 1.8061 - val_accuracy: 0.5765\nEpoch 149/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3323 - accuracy: 0.8892 - val_loss: 1.7748 - val_accuracy: 0.5782\nEpoch 150/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3244 - accuracy: 0.8908 - val_loss: 1.8574 - val_accuracy: 0.5701\nEpoch 151/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3186 - accuracy: 0.8919 - val_loss: 1.9067 - val_accuracy: 0.5737\nEpoch 152/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3177 - accuracy: 0.8901 - val_loss: 1.8072 - val_accuracy: 0.5854\nEpoch 153/200\n449/449 [==============================] - 5s 12ms/step - loss: 0.3250 - accuracy: 0.8911 - val_loss: 1.7863 - val_accuracy: 0.5712\nEpoch 154/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3348 - accuracy: 0.8879 - val_loss: 1.7843 - val_accuracy: 0.5687\nEpoch 155/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3164 - accuracy: 0.8931 - val_loss: 1.9376 - val_accuracy: 0.5860\nEpoch 156/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3270 - accuracy: 0.8890 - val_loss: 1.7599 - val_accuracy: 0.5754\nEpoch 157/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3099 - accuracy: 0.8941 - val_loss: 1.8401 - val_accuracy: 0.5779\nEpoch 158/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3122 - accuracy: 0.8950 - val_loss: 1.8790 - val_accuracy: 0.5776\nEpoch 159/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3142 - accuracy: 0.8926 - val_loss: 1.9120 - val_accuracy: 0.5846\nEpoch 160/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3159 - accuracy: 0.8928 - val_loss: 1.7976 - val_accuracy: 0.5832\nEpoch 161/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3216 - accuracy: 0.8933 - val_loss: 1.8065 - val_accuracy: 0.5812\nEpoch 162/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3001 - accuracy: 0.8982 - val_loss: 1.9037 - val_accuracy: 0.5782\nEpoch 163/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3006 - accuracy: 0.8969 - val_loss: 1.8254 - val_accuracy: 0.5804\nEpoch 164/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3131 - accuracy: 0.8917 - val_loss: 1.8787 - val_accuracy: 0.5795\nEpoch 165/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3096 - accuracy: 0.8969 - val_loss: 1.8491 - val_accuracy: 0.5653\nEpoch 166/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3081 - accuracy: 0.8967 - val_loss: 1.8714 - val_accuracy: 0.5734\nEpoch 167/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3072 - accuracy: 0.8972 - val_loss: 1.8956 - val_accuracy: 0.5706\nEpoch 168/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3083 - accuracy: 0.8967 - val_loss: 1.8647 - val_accuracy: 0.5729\nEpoch 169/200\n","name":"stdout"},{"output_type":"stream","text":"449/449 [==============================] - 6s 12ms/step - loss: 0.3042 - accuracy: 0.8978 - val_loss: 1.8514 - val_accuracy: 0.5743\nEpoch 170/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3057 - accuracy: 0.8973 - val_loss: 1.8807 - val_accuracy: 0.5653\nEpoch 171/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3037 - accuracy: 0.8961 - val_loss: 1.9578 - val_accuracy: 0.5681\nEpoch 172/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3182 - accuracy: 0.8948 - val_loss: 1.8882 - val_accuracy: 0.5706\nEpoch 173/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3134 - accuracy: 0.8956 - val_loss: 1.8248 - val_accuracy: 0.5804\nEpoch 174/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3108 - accuracy: 0.8957 - val_loss: 2.0194 - val_accuracy: 0.5776\nEpoch 175/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3001 - accuracy: 0.8980 - val_loss: 1.9269 - val_accuracy: 0.5628\nEpoch 176/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3087 - accuracy: 0.8968 - val_loss: 1.7966 - val_accuracy: 0.5743\nEpoch 177/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.2990 - accuracy: 0.8995 - val_loss: 1.8734 - val_accuracy: 0.5645\nEpoch 178/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.2971 - accuracy: 0.8989 - val_loss: 1.9008 - val_accuracy: 0.5690\nEpoch 179/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.3014 - accuracy: 0.9013 - val_loss: 1.8526 - val_accuracy: 0.5715\nEpoch 180/200\n449/449 [==============================] - 5s 12ms/step - loss: 0.3039 - accuracy: 0.8978 - val_loss: 1.8711 - val_accuracy: 0.5709\nEpoch 181/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3062 - accuracy: 0.8969 - val_loss: 1.8745 - val_accuracy: 0.5717\nEpoch 182/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3015 - accuracy: 0.8988 - val_loss: 1.9367 - val_accuracy: 0.5723\nEpoch 183/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3027 - accuracy: 0.9007 - val_loss: 1.8587 - val_accuracy: 0.5706\nEpoch 184/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.2951 - accuracy: 0.9014 - val_loss: 1.9123 - val_accuracy: 0.5768\nEpoch 185/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.2963 - accuracy: 0.8999 - val_loss: 2.0152 - val_accuracy: 0.5754\nEpoch 186/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.2918 - accuracy: 0.9030 - val_loss: 1.9339 - val_accuracy: 0.5798\nEpoch 187/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.2960 - accuracy: 0.9029 - val_loss: 2.0030 - val_accuracy: 0.5740\nEpoch 188/200\n449/449 [==============================] - 5s 12ms/step - loss: 0.2925 - accuracy: 0.9016 - val_loss: 1.9634 - val_accuracy: 0.5762\nEpoch 189/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.3030 - accuracy: 0.8990 - val_loss: 1.8846 - val_accuracy: 0.5704\nEpoch 190/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.2871 - accuracy: 0.9052 - val_loss: 1.9003 - val_accuracy: 0.5720\nEpoch 191/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.2896 - accuracy: 0.9040 - val_loss: 1.9282 - val_accuracy: 0.5712\nEpoch 192/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.2920 - accuracy: 0.9045 - val_loss: 1.8836 - val_accuracy: 0.5865\nEpoch 193/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.2880 - accuracy: 0.9022 - val_loss: 1.9033 - val_accuracy: 0.5756\nEpoch 194/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.2937 - accuracy: 0.9064 - val_loss: 1.9274 - val_accuracy: 0.5715\nEpoch 195/200\n449/449 [==============================] - 6s 13ms/step - loss: 0.2832 - accuracy: 0.9071 - val_loss: 2.0021 - val_accuracy: 0.5698\nEpoch 196/200\n449/449 [==============================] - 5s 12ms/step - loss: 0.2822 - accuracy: 0.9043 - val_loss: 2.0532 - val_accuracy: 0.5815\nEpoch 197/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.2843 - accuracy: 0.9063 - val_loss: 1.8090 - val_accuracy: 0.5620\nEpoch 198/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.2843 - accuracy: 0.9042 - val_loss: 1.8854 - val_accuracy: 0.5770\nEpoch 199/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.2975 - accuracy: 0.9018 - val_loss: 1.8894 - val_accuracy: 0.5801\nEpoch 200/200\n449/449 [==============================] - 6s 12ms/step - loss: 0.2875 - accuracy: 0.9049 - val_loss: 1.9333 - val_accuracy: 0.5709\n","name":"stdout"},{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f2de8d25710>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"fer_json = model.to_json()\nwith open(\"fer.json\", \"w\") as json_file:\n    json_file.write(fer_json)\nmodel.save_weights(\"fer.h5\")","execution_count":10,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}